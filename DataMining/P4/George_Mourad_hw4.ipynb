{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 484 :: Data Mining :: George Mason University :: Spring 2025\n",
    "\n",
    "\n",
    "# Homework 4: Association Rule Mining\n",
    "\n",
    "- **100 points [6% of your final grade]**\n",
    "- **Due Monday, May 7 by 11:59pm**\n",
    "\n",
    "- *Goals of this homework:* implement the association rule mining process with the Apriori algorithm.\n",
    "\n",
    "- *Submission instructions:* for this homework, you only need to submit to Canvas. Please name your submission **FirstName_Lastname_hw4.ipynb**, so for example, my submission would be something like **Ziwei_Zhu_hw4.ipynb**. Your notebook should be fully executed so that we can see all outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you are going to examine movies using our understanding of association rules. For this part, you need to implement the apriori algorithm, and apply it to a movie rating dataset to find association rules of user-rate-movie behaviors. First, run the next cell to load the dataset we are going to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array of user-movie matrix: shape (11743, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "user_movie_data = np.loadtxt(\"movie_rated.txt\", delimiter=',')\n",
    "print('array of user-movie matrix: shape ' + str(np.shape(user_movie_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this dataset, there are two columns: the first column is the integer ids of users, and the second column is the integer ids of movies. Each row denotes that the user of given user id watched the movie of the given movie id. We are going to treat each user as a transaction, so you will need to collect all the movies that have been watched by a single user as a transaction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you need to implement the apriori algorithm and apply it to this dataset to find association rules of user movie-watching behaviors with **minimum support of 0.2** and **minimum confidence of 0.8**. We know there are many existing implementations of apriori online (check github for some good starting points). You are welcome to read existing codebases and let that inform your approach. \n",
    "\n",
    "**Note: Do not copy-paste any existing code.**\n",
    "\n",
    "**Note: We want your code to have sufficient comments to explain your steps, to show us that you really know what you are doing.**\n",
    "\n",
    "**Note: You should add print statements to print out the intermediate steps of your method -- e.g., the size of the candidate set at each step of the method, the size of the filtered set, and any other important information you think will explain the process of the method.**\n",
    "\n",
    "**Hint: If you implement your algorithm correctly, you should be able to see intermediate result as:**\n",
    "- Candidates of length 1 count: 408\n",
    "- After Pruning count: 21\n",
    "- Candidates of length 2 count: 210\n",
    "- After Pruning 2 count: 36\n",
    "- Candidates of length 3 count: 55\n",
    "- After Pruning 3 count: 12\n",
    "- Candidates of length 4 count: 1\n",
    "- After Pruning 4 count: 0\n",
    "\n",
    "**Hint: \"Candidates of length 1/2/3/4 count\" can be different, depending on what methods you use to generate candidate sets. But, your \"After Pruning count\" should be the same as what is shown above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Candidates of length 1 count: 408\n",
      "- After Pruning 1 count: 21\n",
      "Candidates of length 2 count: 210\n",
      "After Pruning 2-itemsets count: 36\n",
      "Candidates of length 3 count: 55\n",
      "After Pruning 3-itemsets count: 12\n",
      "Candidates of length 4 count: 1\n",
      "After Pruning 4-itemsets count: 0\n",
      "\n",
      "Total association rules generated: 14\n"
     ]
    }
   ],
   "source": [
    "# Write your code\n",
    "#include all the helpful print statements\n",
    "# when you run this block, we want to see all of your intermediate steps\n",
    "# you can save the rules you discover for printing in the following cells (this will help us grade by keeping these separate)\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_frequent_itemsets(transactions, candidates, min_support, total_transactions):\n",
    "    \"\"\"Filter candidate itemsets based on min support.\"\"\"\n",
    "    itemset_count = defaultdict(int)\n",
    "    \n",
    "    # Count how many transactions contain each candidate itemset\n",
    "    for transaction in transactions:\n",
    "        for itemset in candidates:\n",
    "            if itemset.issubset(transaction):\n",
    "                itemset_count[itemset] += 1\n",
    "    \n",
    "    # Compute support and filter\n",
    "    frequent_itemsets = {}\n",
    "    for itemset, count in itemset_count.items():\n",
    "        support = count / total_transactions\n",
    "        if support >= min_support:\n",
    "            frequent_itemsets[itemset] = support\n",
    "    \n",
    "    return frequent_itemsets\n",
    "\n",
    "def generate_new_candidates(prev_frequent_itemsets, k):\n",
    "    \"\"\"Generate candidate itemsets of length k from (k-1)-itemsets.\"\"\"\n",
    "    \"\"\"Generate candidate itemsets of length k from (k-1)-itemsets with pruning.\"\"\"\n",
    "    candidates = set()\n",
    "    prev_itemsets = list(prev_frequent_itemsets)\n",
    "    \n",
    "    for i in range(len(prev_itemsets)):\n",
    "        for j in range(i + 1, len(prev_itemsets)):\n",
    "            l1 = sorted(list(prev_itemsets[i]))\n",
    "            l2 = sorted(list(prev_itemsets[j]))\n",
    "            if l1[:k-2] == l2[:k-2]:  # join step\n",
    "                union = prev_itemsets[i].union(prev_itemsets[j])\n",
    "                if len(union) == k:\n",
    "                    # prune step\n",
    "                    subsets = combinations(union, k - 1)\n",
    "                    if all(frozenset(s) in prev_frequent_itemsets for s in subsets):\n",
    "                        candidates.add(union)\n",
    "    return candidates\n",
    "\n",
    "def generate_association_rules(frequent_itemsets, min_confidence, support_data):\n",
    "    \"\"\"Generate association rules from frequent itemsets.\"\"\"\n",
    "    rules = []\n",
    "    for itemset in frequent_itemsets:\n",
    "        if len(itemset) < 2:\n",
    "            continue\n",
    "        for i in range(1, len(itemset)):\n",
    "            for antecedent in combinations(itemset, i):\n",
    "                antecedent = frozenset(antecedent)\n",
    "                consequent = itemset - antecedent\n",
    "                if consequent:\n",
    "                    confidence = support_data[itemset] / support_data[antecedent]\n",
    "                    if confidence >= min_confidence:\n",
    "                        rules.append((antecedent, consequent, confidence))\n",
    "    return rules\n",
    "\n",
    "def apriori(transactions,min_support, min_confidence):\n",
    "    \"\"\"Main Apriori algorithm.\"\"\"\n",
    "    total_transactions = len(transactions)\n",
    "    transactions = [set(t) for t in transactions]  # ensure each transaction is a set\n",
    "\n",
    "    # Step 1: Generate C1 (1-itemset candidates)\n",
    "    item_counts = defaultdict(int)\n",
    "    for transaction in transactions:\n",
    "        for item in transaction:\n",
    "            item_counts[frozenset([item])] += 1\n",
    "\n",
    "    print(f\"- Candidates of length 1 count: {len(item_counts)}\")\n",
    "\n",
    "    # Step 2: Filter C1 â†’ L1 (frequent 1-itemsets)\n",
    "    L1 = get_frequent_itemsets(transactions, item_counts.keys(), min_support, total_transactions)\n",
    "    print(f\"- After Pruning 1 count: {len(L1)}\")\n",
    "    support_data = {}\n",
    "    support_data.update(L1)\n",
    "\n",
    "    # Initialize the list of frequent itemsets\n",
    "    L = [L1]\n",
    "\n",
    "    k = 2\n",
    "    while True:\n",
    "        prev_L = list(L[-1].keys())  # Use the last frequent itemsets\n",
    "        Ck = generate_new_candidates(prev_L, k)\n",
    "        print(f\"Candidates of length {k} count: {len(Ck)}\")\n",
    "        Lk = get_frequent_itemsets(transactions, Ck, min_support, total_transactions)\n",
    "        print(f\"After Pruning {k}-itemsets count: {len(Lk)}\")\n",
    "\n",
    "        if not Lk:\n",
    "            break\n",
    "        L.append(Lk)\n",
    "        support_data.update(Lk)\n",
    "        k += 1\n",
    "\n",
    "    # Combine all frequent itemsets\n",
    "    all_frequent_itemsets = {}\n",
    "    for freq in L:\n",
    "        all_frequent_itemsets.update(freq)\n",
    "\n",
    "    # Generate rules\n",
    "    rules = generate_association_rules(all_frequent_itemsets.keys(), min_confidence, support_data)\n",
    "    print(f\"\\nTotal association rules generated: {len(rules)}\")\n",
    "\n",
    "    return rules\n",
    "\n",
    "# Convert to DataFrame for easy grouping\n",
    "df = pd.DataFrame(user_movie_data, columns=['user_id', 'movie_id'])\n",
    "\n",
    "# Group by user and collect movie IDs into sets\n",
    "transactions = df.groupby('user_id')['movie_id'].apply(set).tolist()\n",
    "# Run the apriori algorithm\n",
    "rules = apriori(transactions, 0.2, 0.8)\n",
    "\n",
    "# Load the movie names to convert movie IDs into names\n",
    "movies_df = pd.read_csv('movies.csv')\n",
    "movie_id_to_name = dict(zip(movies_df['movieId'], movies_df['movie_name']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, print your final association rules in the following format:\n",
    "\n",
    "**movie_name_1, movie_name_2, ... --> movie_name_k**\n",
    "\n",
    "where the movie names can be fetched by joining the movieId with the file 'movies.csv'. For example, one rule that you should find is:\n",
    "\n",
    "**Jurassic Park (1993), Back to the Future (1985) --> Star Wars: Episode IV - A New Hope (1977)**\n",
    "\n",
    "**Hint: You may need to use the Pandas library to load and process the movies.csv file, such as use pandas.read_csv() to load the data. https://pandas.pydata.org/pandas-docs/dev/user_guide/10min.html is a good place to learn the basics about Pandas.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hint: if you implement the algorith correctly, you will find 14 rules in total:**\n",
    "- Back to the Future (1985), Schindler's List (1993) -> Star Wars: Episode IV - A New Hope (1977)\n",
    "- Godfather, The (1972), Godfather: Part II, The (1974) -> Star Wars: Episode IV - A New Hope (1977)\n",
    "- Godfather: Part II, The (1974) -> Godfather, The (1972)\n",
    "- Groundhog Day (1993), Princess Bride, The (1987) -> Back to the Future (1985)\n",
    "- Groundhog Day (1993), Star Wars: Episode IV - A New Hope (1977) -> Back to the Future (1985)\n",
    "- Jaws (1975) -> Star Wars: Episode IV - A New Hope (1977)\n",
    "- Jurassic Park (1993), Back to the Future (1985) -> Star Wars: Episode IV - A New Hope (1977)\n",
    "- Jurassic Park (1993), Princess Bride, The (1987) -> Star Wars: Episode IV - A New Hope (1977)\n",
    "- Jurassic Park (1993), Saving Private Ryan (1998) -> Star Wars: Episode IV - A New Hope (1977)\n",
    "- Princess Bride, The (1987), Back to the Future (1985) -> Star Wars: Episode IV - A New Hope (1977)\n",
    "- Saving Private Ryan (1998), Back to the Future (1985) -> Star Wars: Episode IV - A New Hope (1977)\n",
    "- Saving Private Ryan (1998), Princess Bride, The (1987) -> Star Wars: Episode IV - A New Hope (1977)\n",
    "- Star Wars: Episode IV - A New Hope (1977), Godfather: Part II, The (1974) -> Godfather, The (1972)\n",
    "- Star Wars: Episode IV - A New Hope (1977), Princess Bride, The (1987) -> Back to the Future (1985)\n",
    "\n",
    "**Hint: the order of movies on the lefthand side does not matter, i.e., A,B->C is the same as B,A->C.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Godfather: Part II, The (1974) --> Godfather, The (1972)\n",
      "Jaws (1975) --> Star Wars: Episode IV - A New Hope (1977)\n",
      "Saving Private Ryan (1998), Back to the Future (1985) --> Star Wars: Episode IV - A New Hope (1977)\n",
      "Back to the Future (1985), Schindler's List (1993) --> Star Wars: Episode IV - A New Hope (1977)\n",
      "Groundhog Day (1993), Princess Bride, The (1987) --> Back to the Future (1985)\n",
      "Saving Private Ryan (1998), Princess Bride, The (1987) --> Star Wars: Episode IV - A New Hope (1977)\n",
      "Star Wars: Episode IV - A New Hope (1977), Princess Bride, The (1987) --> Back to the Future (1985)\n",
      "Princess Bride, The (1987), Back to the Future (1985) --> Star Wars: Episode IV - A New Hope (1977)\n",
      "Groundhog Day (1993), Star Wars: Episode IV - A New Hope (1977) --> Back to the Future (1985)\n",
      "Jurassic Park (1993), Back to the Future (1985) --> Star Wars: Episode IV - A New Hope (1977)\n",
      "Jurassic Park (1993), Princess Bride, The (1987) --> Star Wars: Episode IV - A New Hope (1977)\n",
      "Godfather, The (1972), Godfather: Part II, The (1974) --> Star Wars: Episode IV - A New Hope (1977)\n",
      "Star Wars: Episode IV - A New Hope (1977), Godfather: Part II, The (1974) --> Godfather, The (1972)\n",
      "Jurassic Park (1993), Saving Private Ryan (1998) --> Star Wars: Episode IV - A New Hope (1977)\n"
     ]
    }
   ],
   "source": [
    "# Write your code to print out the rules\n",
    "for antecedent, consequent, confidence in rules:\n",
    "    antecedent_movies = [movie_id_to_name[movie_id] for movie_id in antecedent]\n",
    "    consequent_movies = [movie_id_to_name[movie_id] for movie_id in consequent]\n",
    "    \n",
    "    # Format the rule\n",
    "    rule = f\"{', '.join(antecedent_movies)} --> {', '.join(consequent_movies)}\"\n",
    "    print(rule)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
